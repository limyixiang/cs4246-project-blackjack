{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf04215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        lr_bet: float,\n",
    "        lr_play: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialize a SARSA agent.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            lr_bet: How quickly to update Q-values in the betting phase (0-1)\n",
    "            lr_play: How quickly to update Q-values in the playing phase (0-1)\n",
    "            initial_epsilon: Starting exploration rate (usually 1.0)\n",
    "            epsilon_decay: How much to reduce epsilon each episode\n",
    "            final_epsilon: Minimum exploration rate (usually 0.1)\n",
    "            discount_factor: How much to value future rewards (0-1)\n",
    "        \"\"\"\n",
    "        base = getattr(env, \"unwrapped\", env)\n",
    "        self.env = env\n",
    "\n",
    "        # Spaces\n",
    "        self.n_bets = int(base.n_bets)\n",
    "        self.n_tc = int(base.observation_space.spaces[3].n)\n",
    "\n",
    "        # Q_bet[tc_bucket, n_bets]\n",
    "        self.Q_bet = np.zeros((self.n_tc, self.n_bets), dtype=np.float32)\n",
    "\n",
    "        # Q_play[player_sum(32), dealer(11), usable(2), tc_bucket, 2]\n",
    "        self.Q_play = np.zeros((32, 11, 2, self.n_tc, 2), dtype=np.float32)\n",
    "\n",
    "        self.lr_bet = lr_bet\n",
    "        self.lr_play = lr_play\n",
    "        self.discount_factor = discount_factor  # How much we care about future rewards\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error_bet = []\n",
    "        self.training_error_play = []\n",
    "\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    # ---------- indexing helpers ----------\n",
    "    @staticmethod\n",
    "    def _unpack(obs):\n",
    "        # obs = (psum, dealer, usable, tc_idx, phase)\n",
    "        return int(obs[0]), int(obs[1]), int(obs[2]), int(obs[3]), int(obs[4])\n",
    "\n",
    "    def _idxs_play(self, obs):\n",
    "        # obs = (psum, dealer, usable, tc_idx, phase)\n",
    "        ps, dv, ua, tc, _ = self._unpack(obs)\n",
    "        return ps, dv, ua, tc\n",
    "\n",
    "    # ---------- ε-greedy policies ----------\n",
    "    def select_bet(self, obs):\n",
    "        # obs phase must be 0\n",
    "        tc_idx = int(obs[3])\n",
    "        q = self.Q_bet[tc_idx]\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return int(self.rng.integers(self.n_bets))\n",
    "        # argmax with random tie-break\n",
    "        m = q.max(); idxs = np.flatnonzero(q == m)\n",
    "        return int(self.rng.choice(idxs))\n",
    "\n",
    "    def select_play(self, obs):\n",
    "        # obs phase must be 1\n",
    "        ps, dv, ua, tc = self._idxs_play(obs)\n",
    "        q = self.Q_play[ps, dv, ua, tc]\n",
    "        if self.rng.random() < self.epsilon:\n",
    "            return int(self.rng.integers(2))   # 0=stick, 1=hit\n",
    "        m = q.max(); idxs = np.flatnonzero(q == m)\n",
    "        return int(self.rng.choice(idxs))\n",
    "    \n",
    "    # ---------- SARSA updates ----------\n",
    "    def update_bet_sarsa(self, s0, a_bet, r0, s1, a1_play):\n",
    "        # r0 is 0 in env; bootstrap through first play-state/action\n",
    "        tc0 = int(s0[3])\n",
    "        qsa = self.Q_bet[tc0, a_bet]\n",
    "        target = r0 + self.discount_factor * self.Q_play[self._idxs_play(s1)][a1_play]\n",
    "        td = target - qsa\n",
    "        self.Q_bet[tc0, a_bet] += self.lr_bet * td\n",
    "        self.training_error_bet.append(td)\n",
    "\n",
    "    def update_bet_mc(self, s0, a_bet, G):\n",
    "        # optional Monte-Carlo kick at episode end with full return\n",
    "        tc0 = int(s0[3])\n",
    "        qsa = self.Q_bet[tc0, a_bet]\n",
    "        td = G - qsa\n",
    "        self.Q_bet[tc0, a_bet] += self.lr_bet * td\n",
    "        self.training_error_bet.append(td)\n",
    "\n",
    "    def update_play_sarsa(self, s, a, r, done, s_next=None, a_next=None):\n",
    "        ps, dv, ua, tc = self._idxs_play(s)\n",
    "        qsa = self.Q_play[ps, dv, ua, tc, a]\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            ps2, dv2, ua2, tc2 = self._idxs_play(s_next)\n",
    "            target = r + self.discount_factor * self.Q_play[ps2, dv2, ua2, tc2, a_next]\n",
    "        td = target - qsa\n",
    "        self.Q_play[ps, dv, ua, tc, a] += self.lr_play * td\n",
    "        self.training_error_play.append(td)\n",
    "\n",
    "    # ---------- epsilon schedule ----------\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    # ---------- greedy (masked) for evaluation ----------\n",
    "    def greedy_bet(self, obs):\n",
    "        tc = int(obs[3]); q = self.Q_bet[tc]\n",
    "        m = q.max(); idxs = np.flatnonzero(q == m)\n",
    "        return int(self.rng.choice(idxs))\n",
    "\n",
    "    def greedy_play(self, obs):\n",
    "        ps, dv, ua, tc = self._idxs_play(obs)\n",
    "        q = self.Q_play[ps, dv, ua, tc]\n",
    "        m = q.max(); idxs = np.flatnonzero(q == m)\n",
    "        return int(self.rng.choice(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import BlackjackEnv\n",
    "\n",
    "# Training hyperparameters\n",
    "# learning_rate = 0.01        # How fast to learn (higher = faster but less stable) \n",
    "lr_bet = 0.2\n",
    "lr_play = 0.01\n",
    "n_episodes = 5_000_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "final_epsilon = 0.1         # Always keep some exploration\n",
    "\n",
    "# Create environment and agent\n",
    "env = BlackjackEnv(num_decks=2, tc_min=-20, tc_max=20, natural=True)\n",
    "tc_min, tc_max = env.tc_min, env.tc_max\n",
    "print(tc_min)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    env=env,\n",
    "    lr_bet=lr_bet,\n",
    "    lr_play=lr_play,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d94fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "n_buckets = env.observation_space.spaces[3].n\n",
    "hist_start = np.zeros(n_buckets, dtype=np.int64)\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    # ----- Phase 0: choose bet -----\n",
    "    s0, _ = env.reset()\n",
    "    tc_idx = s0[3]                     # integer in [0, n_buckets-1]\n",
    "    hist_start[tc_idx] += 1\n",
    "    a_bet = agent.select_bet(s0)\n",
    "    s1, r0, term, trunc, _ = env.step(a_bet)\n",
    "    assert s1[4] == 1 and not (term or trunc)\n",
    "    a1 = agent.select_play(s1)\n",
    "\n",
    "    # SARSA update for the bet\n",
    "    # agent.update_bet_sarsa(s0, a_bet, r0, s1, a1)\n",
    "    \n",
    "    # ----- Phase 1: play hand -----\n",
    "    G = 0.0\n",
    "    done = False\n",
    "    s = s1; a = a1\n",
    "    while not done:\n",
    "        s_next, reward, terminated, truncated, _ = env.step(a)\n",
    "        G += reward\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            agent.update_play_sarsa(s, a, reward, True)\n",
    "            break\n",
    "        a_next = agent.select_play(s_next)\n",
    "        agent.update_play_sarsa(s, a, reward, False, s_next, a_next)\n",
    "        s, a = s_next, a_next\n",
    "\n",
    "    agent.update_bet_mc(s0, a_bet, G)\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "# Pretty print\n",
    "names = getattr(env.unwrapped, \"tc_bucket_names\", (\"≤-3\",\"-2\",\"-1\",\"0\",\"+1\",\"+2\",\"≥+3\"))\n",
    "labels = np.array(names)\n",
    "for b, c in zip(labels, hist_start):\n",
    "    print(f\"TC {b}: {c}\")\n",
    "print(\"coverage %:\", np.round(100 * hist_start / hist_start.sum(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_moving_avgs(arr, window, convolution_mode):\n",
    "    \"\"\"Compute moving average to smooth noisy data.\"\"\"\n",
    "    return np.convolve(\n",
    "        np.array(arr).flatten(),\n",
    "        np.ones(window),\n",
    "        mode=convolution_mode\n",
    "    ) / window\n",
    "\n",
    "# Smooth over a 500-episode window\n",
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "\n",
    "# Episode rewards (win/loss performance)\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "reward_moving_average = get_moving_avgs(\n",
    "    env.return_queue,\n",
    "    rolling_length,\n",
    "    \"valid\"\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[0].set_ylabel(\"Average Reward\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "\n",
    "# Episode lengths (how many actions per hand)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = get_moving_avgs(\n",
    "    env.length_queue,\n",
    "    rolling_length,\n",
    "    \"valid\"\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[1].set_ylabel(\"Average Episode Length\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "\n",
    "# Training error (how much we're still learning)\n",
    "axs[2].set_title(\"Training Error (Bet)\")\n",
    "training_error_bet_moving_average = get_moving_avgs(\n",
    "    agent.training_error_bet,\n",
    "    rolling_length,\n",
    "    \"same\"\n",
    ")\n",
    "axs[2].plot(range(len(training_error_bet_moving_average)), training_error_bet_moving_average)\n",
    "axs[2].set_ylabel(\"Temporal Difference Error\")\n",
    "axs[2].set_xlabel(\"Step\")\n",
    "\n",
    "axs[3].set_title(\"Training Error (Play)\")\n",
    "training_error_play_moving_average = get_moving_avgs(\n",
    "    agent.training_error_play,\n",
    "    rolling_length,\n",
    "    \"same\"\n",
    ")\n",
    "axs[3].plot(range(len(training_error_play_moving_average)), training_error_play_moving_average)\n",
    "axs[3].set_ylabel(\"Temporal Difference Error\")\n",
    "axs[3].set_xlabel(\"Step\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save as PNG into your Drive folder\n",
    "plt.savefig('SARSA_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the trained agent\n",
    "# def test_agent(agent, env, num_episodes=1000):\n",
    "#     \"\"\"Test agent performance without learning or exploration.\"\"\"\n",
    "#     total_rewards = []\n",
    "\n",
    "#     # Temporarily disable exploration for testing\n",
    "#     old_epsilon = agent.epsilon\n",
    "#     agent.epsilon = 0.0  # Pure exploitation\n",
    "\n",
    "#     for _ in range(num_episodes):\n",
    "#         obs, info = env.reset()\n",
    "#         episode_reward = 0\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             action = agent.get_action(obs)\n",
    "#             obs, reward, terminated, truncated, info = env.step(action)\n",
    "#             episode_reward += reward\n",
    "#             done = terminated or truncated\n",
    "\n",
    "#         total_rewards.append(episode_reward)\n",
    "\n",
    "#     # Restore original epsilon\n",
    "#     agent.epsilon = old_epsilon\n",
    "\n",
    "#     win_rate = np.mean(np.array(total_rewards) > 0)\n",
    "#     average_reward = np.mean(total_rewards)\n",
    "\n",
    "#     print(f\"Test Results over {num_episodes} episodes:\")\n",
    "#     print(f\"Win Rate: {win_rate:.1%}\")\n",
    "#     print(f\"Average Reward: {average_reward:.3f}\")\n",
    "#     print(f\"Standard Deviation: {np.std(total_rewards):.3f}\")\n",
    "\n",
    "# # Test your agent\n",
    "# test_agent(agent, env, num_episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bankroll(agent, env, episodes=200_000, rng=None):\n",
    "    rng = rng or np.random.default_rng()\n",
    "    returns = np.empty(episodes, dtype=np.float64)\n",
    "    total_bet = 0.0\n",
    "    wins = losses = pushes = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s0, _ = env.reset()                    # phase 0 (bet)\n",
    "        a_bet = agent.greedy_bet(s0)\n",
    "        bet = float(env.unwrapped.bet_multipliers[int(a_bet) % env.unwrapped.n_bets])\n",
    "        total_bet += bet\n",
    "\n",
    "        # play hand\n",
    "        s, _ = env.step(a_bet)[:2]\n",
    "        done = False; G = 0.0\n",
    "        while not done:\n",
    "            a = agent.greedy_play(s)\n",
    "            s, r, term, trunc, _ = env.step(a)\n",
    "            G += r; done = term or trunc\n",
    "\n",
    "        returns[ep] = G\n",
    "        if G > 0: wins += 1\n",
    "        elif G < 0: losses += 1\n",
    "        else: pushes += 1\n",
    "\n",
    "    ev = returns.mean()                        # units/hand\n",
    "    se = returns.std(ddof=1) / np.sqrt(episodes)\n",
    "    ci = (ev - 1.96*se, ev + 1.96*se)\n",
    "    roi = ev / (total_bet / episodes)          # profit per unit bet\n",
    "\n",
    "    summary = {\n",
    "        \"hands\": episodes,\n",
    "        \"bankroll_change\": returns.sum(),\n",
    "        \"ev_per_hand\": ev,\n",
    "        \"ev_95%_CI\": ci,\n",
    "        \"avg_bet\": total_bet / episodes,\n",
    "        \"roi_per_hand\": roi,\n",
    "        \"win_rate\": wins / episodes,\n",
    "        \"loss_rate\": losses / episodes,\n",
    "        \"push_rate\": pushes / episodes,\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "results = evaluate_bankroll(agent, env)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def eval_avg_return_by_tc(agent, env, episodes=200_000, greedy=True, rng=None):\n",
    "    \"\"\"\n",
    "    Returns (labels, mean, (ci_lo, ci_hi), counts)\n",
    "      labels: 1D array of TC values (tc_min..tc_max)\n",
    "      mean:   avg return per hand for each TC bucket\n",
    "      ci:     95% confidence intervals for each bucket\n",
    "      counts: number of episodes that started in each TC bucket\n",
    "    \"\"\"\n",
    "    rng = rng or np.random.default_rng()\n",
    "    base = getattr(env, \"unwrapped\", env)\n",
    "    n_buckets = env.observation_space.spaces[3].n\n",
    "    names = getattr(env.unwrapped, \"tc_bucket_names\", (\"≤-3\",\"-2\",\"-1\",\"0\",\"+1\",\"+2\",\"≥+3\"))\n",
    "    labels = np.array(names)  # for pretty ticks\n",
    "    n_buckets = env.observation_space.spaces[3].n\n",
    "\n",
    "    ret_sum   = np.zeros(n_buckets, dtype=np.float64)\n",
    "    ret_sumsq = np.zeros(n_buckets, dtype=np.float64)\n",
    "    counts    = np.zeros(n_buckets, dtype=np.int64)\n",
    "\n",
    "    # freeze exploration for eval (if your agent has epsilon)\n",
    "    old_eps = getattr(agent, \"epsilon\", None)\n",
    "    if greedy and old_eps is not None:\n",
    "        agent.epsilon = 0.0\n",
    "\n",
    "    def pick_action(obs):\n",
    "        # Single-table agent with masking helpers\n",
    "        if hasattr(agent, \"_valid_action_indices\") and hasattr(agent, \"_argmax_over\"):\n",
    "            if greedy:\n",
    "                idxs = agent._valid_action_indices()\n",
    "                q = agent.q_values[obs]\n",
    "                return int(agent._argmax_over(q, idxs))\n",
    "            else:\n",
    "                return int(agent.get_action(obs))\n",
    "        # Split agent API (two Q tables)\n",
    "        if hasattr(agent, \"greedy_bet\") and hasattr(agent, \"greedy_play\"):\n",
    "            return int(agent.greedy_bet(obs) if obs[4] == 0 else agent.greedy_play(obs))\n",
    "        # Fallback\n",
    "        return int(agent.get_action(obs))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()           # phase 0; obs[3] is the TC bucket index\n",
    "        tc_idx = int(obs[3])\n",
    "\n",
    "        # Play the full hand with the current policy (greedy or epsilon-greedy)\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        while not done:\n",
    "            a = pick_action(obs)\n",
    "            obs, r, term, trunc, _ = env.step(a)\n",
    "            G += r\n",
    "            done = term or trunc\n",
    "\n",
    "        # Aggregate by start-of-hand TC\n",
    "        ret_sum[tc_idx]   += G\n",
    "        ret_sumsq[tc_idx] += G * G\n",
    "        counts[tc_idx]    += 1\n",
    "\n",
    "    if greedy and old_eps is not None:\n",
    "        agent.epsilon = old_eps\n",
    "\n",
    "    # Means and 95% CIs per bucket\n",
    "    denom = np.maximum(counts, 1)\n",
    "    mean = ret_sum / denom\n",
    "    var  = (ret_sumsq / denom) - mean**2\n",
    "    se   = np.sqrt(np.maximum(var, 0.0) / denom)\n",
    "    ci_lo = mean - 1.96 * se\n",
    "    ci_hi = mean + 1.96 * se\n",
    "    return labels, mean, (ci_lo, ci_hi), counts\n",
    "\n",
    "def plot_avg_return_by_tc(labels, mean, ci, counts, min_visits=1000):\n",
    "    ci_lo, ci_hi = ci\n",
    "    mask = counts >= min_visits\n",
    "    x = np.arange(n_buckets); y = mean[mask]\n",
    "    yerr = np.vstack((y - ci_lo[mask], ci_hi[mask] - y))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(x, y, edgecolor='k')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.errorbar(x, y, yerr=yerr, fmt='none', capsize=2, linewidth=1)\n",
    "    plt.axhline(0, color='k', linewidth=0.8)\n",
    "    plt.xlabel('True Count')\n",
    "    plt.ylabel('Average Return per Episode')\n",
    "    plt.title('Average Return by True Count')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "labels, mean, ci, counts = eval_avg_return_by_tc(agent, env, episodes=200_000, greedy=True)\n",
    "for L, m, n in zip(labels, mean, counts):\n",
    "    if n: print(f\"TC {L}: mean={m: .4f}  n={n}\")\n",
    "plot_avg_return_by_tc(labels, mean, ci, counts, min_visits=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83032a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned bet multiplier per true count (phase 0) and visualize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper: argmax with deterministic tie-break (first max)\n",
    "def _argmax(q: np.ndarray) -> int:\n",
    "    return int(np.argmax(q))\n",
    "\n",
    "base_env = getattr(env, 'unwrapped', env)\n",
    "# Determine labels that match the number of TC buckets\n",
    "n_buckets = int(base_env.observation_space.spaces[3].n)\n",
    "n_bets = int(getattr(base_env, 'n_bets', 2))\n",
    "maybe_names = getattr(base_env, 'tc_bucket_names', None)\n",
    "if maybe_names is not None and len(maybe_names) == n_buckets:\n",
    "    labels = np.array(list(maybe_names))  # pretty string labels (e.g., \"≤-3\"..\"≥+3\")\n",
    "else:\n",
    "    labels = np.arange(int(base_env.tc_min), int(base_env.tc_max) + 1)\n",
    "\n",
    "rows = []\n",
    "for idx in range(n_buckets):\n",
    "    # Phase-0 observation: before any cards are dealt\n",
    "    s0 = (0, 0, 0, idx, 0)\n",
    "    # Use the dedicated betting Q-table for this TC bucket\n",
    "    q_bets = np.asarray(agent.Q_bet[idx], dtype=float)\n",
    "    best = _argmax(q_bets)\n",
    "    mult = float(base_env.bet_multipliers[best])\n",
    "    visits = (hist_start[idx] if 'hist_start' in globals() else np.nan)\n",
    "    rows.append({\n",
    "        'TC_idx': int(idx),\n",
    "        'TC': labels[idx],\n",
    "        'best_bet_action': int(best),\n",
    "        'bet_multiplier': mult,\n",
    "        'visits_at_start': int(visits) if not np.isnan(visits) else None,\n",
    "        'Q_bets': q_bets.copy(),\n",
    "    })\n",
    "\n",
    "bet_df = pd.DataFrame(rows).sort_values('TC_idx').reset_index(drop=True)\n",
    "display(bet_df)\n",
    "\n",
    "# Bar chart of chosen multiplier vs TC (optionally filter low-visit buckets)\n",
    "min_visits = 0  # set to e.g. 1000 to hide low-data bins\n",
    "plot_df = bet_df\n",
    "if 'hist_start' in globals() and min_visits > 0:\n",
    "    plot_df = bet_df[bet_df['visits_at_start'].fillna(0) >= min_visits]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(plot_df['TC'], plot_df['bet_multiplier'], color='tab:green', edgecolor='k')\n",
    "plt.xlabel('True Count')\n",
    "plt.ylabel('Greedy Bet Multiplier')\n",
    "plt.title('Learned Bet Multiplier by True Count (Phase 0)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf37d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
